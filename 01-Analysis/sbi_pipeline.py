# engines/sbi_pipeline.py
import os
import glob
import pickle
import numpy as np
from typing import List, Dict, Optional
from loguru import logger
import faiss
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

class SBIPipeline:
    """SHawn Bio-Intelligence (SBI) Knowledge Pipeline (FAISS Edition)"""
    
    def __init__(self, 
                 onedrive_path: str = "/Users/soohyunglee/Library/CloudStorage/OneDrive-ê°œì¸",
                 db_path: Optional[str] = None):
        self.onedrive_path = onedrive_path
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ db_path ì„¤ì •
        if db_path is None:
            curr_dir = os.path.dirname(os.path.abspath(__file__))
            self.db_path = os.path.join(os.path.dirname(curr_dir), "knowledge_base")
        else:
            self.db_path = db_path
        self.index_file = os.path.join(self.db_path, "faiss_index.bin")
        self.data_file = os.path.join(self.db_path, "knowledge_data.pkl")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        
        # ì¸ë±ìŠ¤ ë° ë°ì´í„° ì´ˆê¸°í™”
        self.index = None
        self.metadata = [] # List of {content, source}
        self.indexed_files = set()
        
        self.load_index()
        logger.info(f"ğŸ§¬ SBI FAISS Pipeline initialized. Monitoring: {onedrive_path}")

    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.index_file) and os.path.exists(self.data_file):
            try:
                self.index = faiss.read_index(self.index_file)
                with open(self.data_file, 'rb') as f:
                    save_data = pickle.load(f)
                    self.metadata = save_data['metadata']
                    self.indexed_files = save_data['indexed_files']
                logger.info(f"ğŸ“ Loaded existing index with {len(self.metadata)} chunks.")
            except Exception as e:
                logger.error(f"âŒ Failed to load index: {e}")
                self._create_new_index()
        else:
            self._create_new_index()

    def _create_new_index(self):
        """ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        dimension = 384 # all-MiniLM-L6-v2 output dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.metadata = []
        self.indexed_files = set()
        logger.info("ğŸ†• Created fresh FAISS index.")

    def save_index(self):
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(self.db_path, exist_ok=True)
        faiss.write_index(self.index, self.index_file)
        with open(self.data_file, 'wb') as f:
            pickle.dump({
                'metadata': self.metadata,
                'indexed_files': self.indexed_files
            }, f)
        logger.success("ğŸ’¾ FAISS index and metadata saved.")

    def load_and_index(self, force: bool = False):
        """ì›ë“œë¼ì´ë¸Œ í´ë” ìŠ¤ìº” ë° ì‹ ê·œ íŒŒì¼ ì¸ë±ì‹± (ë¶€í•˜ ë°©ì§€ ë°°ì¹˜ ì²˜ë¦¬ ì ìš©)"""
        import time
        files = glob.glob(os.path.join(self.onedrive_path, "**/*.pdf"), recursive=True) + \
                glob.glob(os.path.join(self.onedrive_path, "**/*.txt"), recursive=True)
        
        new_files = [f for f in files if f not in self.indexed_files or force]
        if not new_files:
            logger.info("âœ¨ No new files found in OneDrive.")
            return

        logger.info(f"ğŸ“‚ Found {len(new_files)} new files to index. Starting throttled indexing...")
        
        batch_size = 10
        for i in range(0, len(new_files), batch_size):
            batch = new_files[i:i + batch_size]
            logger.info(f"ğŸš€ Processing Batch {i//batch_size + 1}/{(len(new_files)-1)//batch_size + 1} ({len(batch)} files)...")
            
            for file_path in batch:
                try:
                    self._index_file(file_path)
                    self.indexed_files.add(file_path)
                    # íŒŒì¼ ê°„ ì§§ì€ ì§€ì—° (CPU ì¿¨ë§)
                    time.sleep(1)
                except Exception as e:
                    logger.error(f"âŒ Failed to index {file_path}: {e}")
            
            # ë°°ì¹˜ ê°„ ì¤‘ê°„ ì§€ì—° (ë©”ëª¨ë¦¬ ì •ë¦¬ ìœ ë„)
            self.save_index()
            if i + batch_size < len(new_files):
                logger.info("â³ Batch completed. Cooling down for 3 seconds...")
                time.sleep(3)

    def _index_file(self, file_path: str):
        """ë‹¨ì¼ íŒŒì¼ íŒŒì‹± ë° ë²¡í„°í™”"""
        file_name = os.path.basename(file_path)
        logger.info(f"â³ Processing {file_name}...")
        
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        else:
            loader = TextLoader(file_path)
            
        documents = loader.load()
        chunks = self.text_splitter.split_documents(documents)
        
        contents = [chunk.page_content for chunk in chunks]
        embeddings = self.model.encode(contents).astype('float32')
        
        self.index.add(embeddings)
        for content in contents:
            self.metadata.append({"content": content, "source": file_name})
            
        logger.success(f"âœ… Added {len(chunks)} chunks from {file_name}")

    def search(self, query: str, n_results: int = 3) -> List[Dict]:
        """ì§€ì‹ ê²€ìƒ‰"""
        if self.index is None or len(self.metadata) == 0:
            return []
            
        query_vector = self.model.encode([query]).astype('float32')
        distances, indices = self.index.search(query_vector, n_results)
        
        hits = []
        for i, idx in enumerate(indices[0]):
            if idx != -1 and idx < len(self.metadata):
                hits.append({
                    "content": self.metadata[idx]['content'],
                    "source": self.metadata[idx]['source'],
                    "distance": float(distances[0][i])
                })
        return hits

if __name__ == "__main__":
    pipeline = SBIPipeline()
    pipeline.load_and_index()
    
    test_query = "ì˜¤ê°€ë…¸ì´ë“œ"
    results = pipeline.search(test_query)
    print(f"\nğŸ” Search Results for '{test_query}':")
    for hit in results:
        print(f"- [{hit['source']}] {hit['content'][:150]}...")
